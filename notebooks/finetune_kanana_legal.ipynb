{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3K1dQxMS84dP",
   "metadata": {
    "id": "3K1dQxMS84dP"
   },
   "source": [
    "# Korean Legal Data QLoRA Fine-tuning (Kanana Nano 2.1B)\n",
    "\n",
    "This notebook fine-tunes the Kanana Nano 2.1B Instruct model on Korean legal data using QLoRA method.\n",
    "It is structured to execute the entire flow from data loading to training, saving, and simple testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8z6IjJ69BW2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8z6IjJ69BW2",
    "outputId": "e1fc4963-1b88-47c5-857a-0434aaae2573"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cJYoLssS84dQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJYoLssS84dQ",
    "outputId": "522856f8-ee96-4e25-80e9-496978391052"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0. Install Dependencies (Colab default Python3 environment)\n",
    "# ============================================================\n",
    "\n",
    "!pip -q install transformers datasets peft trl bitsandbytes accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "xrhX_GjX84dR",
   "metadata": {
    "id": "xrhX_GjX84dR"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Korean Legal Data Fine-tuning with QLoRA\n",
    "# Base Model: Kanana Nano 2.1B Instruct (Kakao)\n",
    "# ============================================================\n",
    "\n",
    "\"\"\"\n",
    "This script uses QLoRA on Google Colab T4 GPU to\n",
    "fine-tune the Kanana Nano 2.1B model on Korean legal data.\n",
    "\n",
    "Model: kakaocorp/kanana-nano-2.1b-instruct\n",
    "Dataset: Legal terminology + case law + custom data\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zxqhMpy89L_C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxqhMpy89L_C",
    "outputId": "f786fe8e-9477-49da-feee-f13e1411ea33"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive save base path: /content/drive/MyDrive/fine_tuned_models\n",
      "Exists: True\n"
     ]
    }
   ],
   "source": [
    "DRIVE_BASE_DIR = \"/content/drive/MyDrive/fine_tuned_models\"\n",
    "Path(DRIVE_BASE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Drive save base path: {DRIVE_BASE_DIR}\")\n",
    "print(f\"Exists: {Path(DRIVE_BASE_DIR).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "N766SG9a84dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N766SG9a84dS",
    "outputId": "22bf3640-fd42-4fd3-9728-c5723c849341"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Thu Jan 15 20:25:56 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   31C    P0             54W /  400W |       5MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0-1. GPU Check\n",
    "# ============================================================\n",
    "\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9k7mE1f484dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9k7mE1f484dS",
    "outputId": "7858622f-7e53-4a28-c59e-508f6272a88d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Configuration 완료\n",
      "  Model: kakaocorp/kanana-nano-2.1b-instruct\n",
      "  LoRA rank: 16\n",
      "  Batch size: 4\n",
      "  Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Configuration\n",
    "# ============================================================\n",
    "\n",
    "# Model Configuration\n",
    "MODEL_NAME = \"kakaocorp/kanana-nano-2.1b-instruct\"\n",
    "\n",
    "# Data path\n",
    "# DATA_PATH is no longer needed (loaded directly from Hugging Face)\n",
    "\n",
    "# Output directory (Colab Google Drive path)\n",
    "OUTPUT_DIR = f\"{DRIVE_BASE_DIR}/kanana-legal-finetuned\"\n",
    "LOGS_DIR = \"./logs\"\n",
    "\n",
    "# ============================================================\n",
    "# 2. QLoRA Configuration\n",
    "# ============================================================\n",
    "\n",
    "# 4-bit quantization Configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                        # LoRA rank\n",
    "    lora_alpha=32,               # LoRA alpha\n",
    "    target_modules=[             # Kanana model's attention layers\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 3. Training Hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=LOGS_DIR,\n",
    "\n",
    "    # Training parameters\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Optimizer\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "\n",
    "    # Precision\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "\n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "\n",
    "    # Other\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",  # W&B disabled\n",
    "    run_name=f\"kanana-legal-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    ")\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "print(\"✓ Configuration 완료\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA rank: {lora_config.r}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ryOBWWUo84dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352,
     "referenced_widgets": [
      "7536944fa99f403a93ca2956e3506877",
      "e55d46b74fb4476c88760aad5b3e82d2",
      "1680d5ade4ee4739a8a2b152ecddb264",
      "f63ef0f37d664012aee91e1a2d841ca0",
      "1944b9c2285640549d9b422887c73ad5",
      "acb188554f194382bfb982864f77a77e",
      "19d34f4523954b438d349d29bb29a813",
      "70df210014aa492fbf8780a26aee9a07",
      "7ab9a61eacaf484085df795e98ee23ca",
      "26ed7f1394b64cfcb83fa9cd4adb35c1",
      "9989173106bb49ae8a2bd5feb1b66408",
      "53e09e46e3cb4d8d89651e8c430acb04",
      "877f0cadc67f4ebdbb258f12ecacae47",
      "694bd2cab9004083afb39768158c944f",
      "9aabd556e1bf4891abc20640f67f5fac",
      "a2ed7a22c6554c44ba6ea0d835e2e1cd",
      "1f5bcd596c764c09b0e3ac744d67cbf5",
      "7bdd59ac61b74b2d8b0743ea6b5578af",
      "78ec670c29aa4951bfe089f1b71bc3c4",
      "8828dfd21f444dc590957d7a2e8543ee",
      "3b144135824d4e41a55ae466868053ed",
      "4aeb6c55bd204663a914eba7ce75a6b8",
      "c75f376294e1422d9e3677c0fc46fb44",
      "0ae491a2c3e24b19b7c74ce68da7dce2",
      "f5dc4da41e17462cbe5ed6cc1c305943",
      "061162d9ff3a495fb3d352b6ed5a4400",
      "061b9f93174d4e4baf1b1ac566276862",
      "73719fb0f756463b831ebfc19c3a65e7",
      "f173765da65844faa39ac3c9dddac428",
      "dd2971e09cfc47e6b0314ce66dfc6a4e",
      "f6fe226f181143eda993e3e5246ffea8",
      "2ae392b9d7ae421cbbafc904caae460a",
      "20ccef07beb940b089efe405719f3290"
     ]
    },
    "id": "ryOBWWUo84dS",
    "outputId": "91d99de4-bde2-40bb-da54-b450cc5c26b7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Data Loading 중...\n",
      "Downloading dataset from Hugging Face...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7536944fa99f403a93ca2956e3506877"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "training_data.json:   0%|          | 0.00/66.7M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "53e09e46e3cb4d8d89651e8c430acb04"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/17484 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c75f376294e1422d9e3677c0fc46fb44"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total samples: 17,484\n",
      "Train: 16,609 samples\n",
      "Validation: 875 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 4. Data Loading\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nData Loading 중...\")\n",
    "print(\"Downloading dataset from Hugging Face...\")\n",
    "\n",
    "# Load dataset from Hugging Face (train split)\n",
    "full_data = load_dataset(\n",
    "    \"flyingcarycoder/korean-legal-terminology\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "print(f\"Total samples: {len(full_data):,}\")\n",
    "\n",
    "# Train/Validation split (95% train, 5% validation)\n",
    "split_dataset = full_data.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "val_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,} samples\")\n",
    "print(f\"Validation: {len(val_dataset):,} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ydnU10Cm84dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428,
     "referenced_widgets": [
      "e626bfa7e16b40c88662abe0aab97911",
      "186cbd45387140fba0f038cf8560bb04",
      "ab28bb997cb44d868321a486be4f1796",
      "c5f04386182b4195aa9f7fb86b30d164",
      "34288ee859d74e16a44d3c0ee82dbef3",
      "89211a1fa27c4c4e8861eaf9ba93ada0",
      "71d0913d93844e079aa41873d68c0f87",
      "ee19e13155c04544b8a3dfc20cc40401",
      "bdb642f4d80f45708241047c3c54d085",
      "c593a6bce15746888e5b73620f694d82",
      "f0c6ee9d7a364cf3b19ede6bfb4c23c7",
      "7f046f22169c408f9fec0c854efee1f6",
      "cbbeb15b593f441cb12a4f5d84e2773e",
      "db0b9c00470d45978824c06b3f5fa312",
      "9671d641ef0941ee8e0c8c67a83b7949",
      "7df1b93304ff4b9cbf18bb9dc25155f8",
      "b571b26ed8da46f1b476c5554bf372e0",
      "e90882ec7aab40d88e12462475b0dc21",
      "a54480fb5e944e30891c6ebb3e61be7c",
      "e848f40ed15f4b6fabaac07e2029e61f",
      "45e84441abbf442ead00ed006fdb9f2b",
      "ff947c9581b847268c5cb5e0630012f1",
      "3d9e5f699e2e45ed8972a867bb788fbe",
      "c577d369079c4fcf850450b8a0e2b125",
      "69cb78c2ef96461f908797ade877f17e",
      "b9862ed2a74d4f5bb59a6ee4427abc53",
      "da0b53e6e0f347aba2e98b1ed7fb9711",
      "2d371dc0e51142de95c547b1fd39b8d1",
      "5b5d3d7dc05c4ff69c88215b90f7894e",
      "264d9aa06c2e42ef81ea14261f9e019b",
      "9a7b2291ee75436595271c1d5fce3049",
      "87394842ac804e4eac2654c75dba1656",
      "f2830fae709a472b91078d8f435c55c0",
      "d788adac6b9843308cbd1f41443f4ebf",
      "79c7cdca30724b3887c828b6d46f060e",
      "efa7fa043954427a881315168165264c",
      "303bdd8ffe474b0bb5ae258d049a952d",
      "c8c0dc51c539421cb6f5440a07c486d9",
      "8c0ddeedea364507b4eb54161d56b0e2",
      "1c868ca5a0fc41a0acbf20c6ceda93b2",
      "e2aa023e6a254e3eacfdebda73cb5a01",
      "a16684331b36476c8e0c9a92e314a051",
      "da43734559594ac2b0fccda99175f5c8",
      "0608e91c3d934377bd0c5987b3b0ce55",
      "cf801518af774ebe8d116ca13953b064",
      "e35bd5d75e904dfab5bf4c78adaf2491",
      "bdbc40283827459e9cdde10f266427dc",
      "676cdad19f074fb4879285951daafb14",
      "91ab39b5ba274dc6a16dee541fd7a291",
      "f2eca8052e10472a84becdd963325713",
      "78342ce47dd748ffbf347043319c294f",
      "4d3f4a3fe16c4e7c90225b1bfb35df69",
      "8ad9598bd5304ca9847ec4c2b74375df",
      "45aaa1ef8d794fbea582d7fface63e65",
      "26fe32a380eb42a08ffd4576b32524d0",
      "fa1129bb9b604e6ca6c726172f3db538",
      "048f08313afa487683147ef15ce2f23b",
      "5d4169946761406592429763c26cecb3",
      "486be3a9cb1a4e1c91aee605f417e99c",
      "ac8cc2d8855749bfa24ffb30a5ca87b7",
      "06ca7ccf7ed94e3d864176007bd8edc2",
      "9b96cb00e49e4609a4548fd99ad69df1",
      "02bd634c51a24a0d937a7ff28a3f61c7",
      "aae7af73e7494ae3b80fddc86a7beb9a",
      "90ab27ef9e5b4da4968e750f36d16fb2",
      "2346f9a085744d6692862d616134a658"
     ]
    },
    "id": "ydnU10Cm84dS",
    "outputId": "617d2768-8b2a-41eb-8895-e27991a4c42b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e626bfa7e16b40c88662abe0aab97911"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f046f22169c408f9fec0c854efee1f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d9e5f699e2e45ed8972a867bb788fbe"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "✓ Tokenizer loaded successfully\n",
      "  Vocab size: 128256\n",
      "\n",
      "Loading model (4-bit quantization)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d788adac6b9843308cbd1f41443f4ebf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf801518af774ebe8d116ca13953b064"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/126 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa1129bb9b604e6ca6c726172f3db538"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "✓ Model loaded successfully\n",
      "  Total parameters: 1,181,468,416\n",
      "  Trainable parameters: 23,003,136\n",
      "  Trainable ratio: 1.95%\n",
      "trainable params: 23,003,136 || all params: 2,109,982,464 || trainable%: 1.0902\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 5. Model and Tokenizer Loading\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Padding Configuration\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"✓ Tokenizer loaded successfully\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "print(\"\\nLoading model (4-bit quantization)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Prepare for K-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\n✓ Model loaded successfully\")\n",
    "print(f\"  Total parameters: {all_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"  Trainable ratio: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "iF8y-lc384dS",
   "metadata": {
    "id": "iF8y-lc384dS"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 6. Prompt Format\n",
    "# ============================================================\n",
    "\n",
    "def format_prompt(sample):\n",
    "    \"\"\"Convert input data to training prompt format\"\"\"\n",
    "    return f\"\"\"### Question:\n",
    "{sample['input']}\n",
    "\n",
    "### Answer:\n",
    "{sample['output']}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "I4_FZsIX84dS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "588817174f5a49138c9baca43d31040c",
      "38eac9ecd388449499fd37c06340ee2a",
      "1771800002f945a58ef28ae35b6bac7f",
      "54b3cc942c9f49c887fb314b16c970a0",
      "51360e01b2fd41baa16cd8ea7e18eef5",
      "4a4915ee0a59440d87fdc1af42a041ec",
      "047cf2b65e854188bdd185b65bad157d",
      "2ddd6f02c3a7401490c58ad264e7af3a",
      "be61b32ace3c4fd995a449365a2cfb08",
      "248926a4682645c3a21d4a8cc9bdef63",
      "97b908097a9b4a5b9d719c0088612898",
      "c5d558edce9545138c089f3a284bdfe2",
      "2da4502021374887964fdb9c80977f0b",
      "b9a902bcd71e4e67a53f80e2632c4a8f",
      "ce8896acf6dd41e387eadba09ebcf72c",
      "ebfa9676bddc4ee7b36e5747b79853ff",
      "c303c1c3bfe349b7974dad3eed000d65",
      "a7a73fd437444f94a30f798a69de9d0c",
      "d559cb48aed24b3fa32808a2bdddd24a",
      "85bda7d539b2481fbb7c927eac7fc4b2",
      "8417f22fa0414b9f870b9e370abf200f",
      "0d582be14afb48f2a58e63d9c890d0e3",
      "7fcb490eee3944b594ae23be07fc4e10",
      "d1dd3122b981434b9d2f163033e68f1a",
      "646403023bc94fa9af8315b6c95517a8",
      "8750e5f5601c4d75bdb29847296d48cf",
      "319b2fbcf4b24723aa0bd7ae5c7ac21d",
      "37fc645a0c8d4324bbd187d6b7f7b406",
      "30a9f4bd21794e4fa83566b4c8322077",
      "8ea1e49d9b374a98ab1d7e43d88635f9",
      "b2b94e79f60446368462242076fd25ef",
      "d9469f0271c54d0fa0e83c59f0cf6a6a",
      "f320aeb7d9464340bb19cd51a58de373",
      "f9ea0a0975094015ad0ea968f1109e07",
      "467998c1f0524755a0406b74e9068aca",
      "7c503392ed50483797c3df4c03d858bd",
      "58ba218fec154d28a37cfcc0099fb8f7",
      "e4c6de80efcb45bb87938c8d45d4678a",
      "45818046d477457e99b8b0bbf2573f25",
      "07c6884d6a974072826a84b5c569d268",
      "06a2686e79e9486095ea08b3f0223308",
      "cc5bc2d0dce044d88271f96439e101dc",
      "93c2026d4c1a4abab5059b8499ccd007",
      "1a1fbc4494e34d42b9d47d2520874b5b",
      "b2eabe428b8945a7b2b313942e345865",
      "1298b35045f6460daaa26fbb76530f6f",
      "76c396cea86948d69689517897bfbe79",
      "4dc93d82e0d740938824b9e57ae8b69d",
      "adfcd1e4c048414a89acd31d19bb20c4",
      "131fd1e691fa44c0ad108a9c50109896",
      "bdb18836168f4d2dbc51724845809644",
      "cf0e2def13a045f8a3deed5b29a57a3f",
      "dfc01534e8b14de1ba21bcac6ae47f5e",
      "7d68d846ed9b4f76a6912dd22b40f5a0",
      "2563fdf0fb194665a88038f5a8de476c",
      "33b1f3c62c6d4647a159d56e47f0c6d5",
      "803a545f127d48d4bc6c9136bf5d7a86",
      "12110423e5cd45b5ad0998ae14936a88",
      "9112a454a83a4c0d9537c26961cd8a38",
      "2072673aa78d4025aaf2481587d2832d",
      "1b0ab65751554ed8bf1c8b2428163900",
      "807e58271fe44ac78e0dfd942a27ebb0",
      "9569f15e779b4ed5bd5ad8088d68e150",
      "96b2fd2299f64dabbe3f0e9fbd6161c4",
      "b70c2bac83c24a8aa9fec1835f2fe331",
      "4ee8af1f98c74c5fb070a2605ed6cd64",
      "777626fa886e48109236556bd0f0c53e",
      "e2dd573a0098449b9673d39f4ad44383",
      "a29a378b9a284b5b97d3044077f69a0b",
      "52bf769b4c0f46c08a622cf7438715a1",
      "9715cd989cac45a093c6f3c4f2e3717f",
      "316dc2fd0b81455191f3f1d82f99dc11",
      "fa7792c1e3ef4814886ae0bc4c712cf8",
      "27e46a00b18c4c5e91135764545c6198",
      "23ba8a486c0e4ed492f33e3c501291ea",
      "ddbe1a14459f4c65aa6042695470ffd7",
      "fd55321ca17b430c948022556c5dcf1d",
      "71e0ad75140c427abc3a607bc4b7c454",
      "3265494725c7490b91b8ec784ccebc02",
      "922047b224f84afda09a97cfac3d9582",
      "078633d4d85149699cc1ae8e24cae721",
      "7ed1642e02f24f81a4e7607735241928",
      "537282e9c55f4ed4bf5bfb425d68b4ec",
      "618eeea249944e82a71d7d9edfac9f64",
      "6892f12c9a3c4060ada0893532d8cd66",
      "a4e205a52ef54f7eb04b7e395261f88c",
      "50094c512df34df898b3030338b7aff3",
      "35ab5aed80234eb58e5cd228e106cc5c"
     ]
    },
    "id": "I4_FZsIX84dS",
    "outputId": "4c53689b-ba28-4371-dfa8-60e624b4f0cb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/16609 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "588817174f5a49138c9baca43d31040c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/16609 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c5d558edce9545138c089f3a284bdfe2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/16609 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fcb490eee3944b594ae23be07fc4e10"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/16609 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9ea0a0975094015ad0ea968f1109e07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/875 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2eabe428b8945a7b2b313942e345865"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/875 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33b1f3c62c6d4647a159d56e47f0c6d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/875 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "777626fa886e48109236556bd0f0c53e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/875 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71e0ad75140c427abc3a607bc4b7c454"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 128009, 'pad_token_id': 128001}.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "Fine-tuning 시작!\n",
      "============================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3117' max='3117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3117/3117 2:53:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.347400</td>\n",
       "      <td>1.348448</td>\n",
       "      <td>1.278470</td>\n",
       "      <td>963863.000000</td>\n",
       "      <td>0.670080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.282038</td>\n",
       "      <td>1.214781</td>\n",
       "      <td>1925369.000000</td>\n",
       "      <td>0.683275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.282600</td>\n",
       "      <td>1.244442</td>\n",
       "      <td>1.246762</td>\n",
       "      <td>2874206.000000</td>\n",
       "      <td>0.688552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.293200</td>\n",
       "      <td>1.211675</td>\n",
       "      <td>1.186833</td>\n",
       "      <td>3799669.000000</td>\n",
       "      <td>0.692966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.288900</td>\n",
       "      <td>1.202000</td>\n",
       "      <td>1.189427</td>\n",
       "      <td>4732602.000000</td>\n",
       "      <td>0.695191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.286300</td>\n",
       "      <td>1.198944</td>\n",
       "      <td>1.224393</td>\n",
       "      <td>5695277.000000</td>\n",
       "      <td>0.696299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.242900</td>\n",
       "      <td>1.180754</td>\n",
       "      <td>1.214276</td>\n",
       "      <td>6629521.000000</td>\n",
       "      <td>0.699385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.234700</td>\n",
       "      <td>1.167861</td>\n",
       "      <td>1.171825</td>\n",
       "      <td>7585008.000000</td>\n",
       "      <td>0.701624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.249900</td>\n",
       "      <td>1.157248</td>\n",
       "      <td>1.168674</td>\n",
       "      <td>8552583.000000</td>\n",
       "      <td>0.703447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.253400</td>\n",
       "      <td>1.146654</td>\n",
       "      <td>1.145106</td>\n",
       "      <td>9502313.000000</td>\n",
       "      <td>0.704891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.863300</td>\n",
       "      <td>1.140149</td>\n",
       "      <td>1.041027</td>\n",
       "      <td>10536205.000000</td>\n",
       "      <td>0.709767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.865400</td>\n",
       "      <td>1.137534</td>\n",
       "      <td>1.056608</td>\n",
       "      <td>11481453.000000</td>\n",
       "      <td>0.710234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.831000</td>\n",
       "      <td>1.131835</td>\n",
       "      <td>1.026597</td>\n",
       "      <td>12429335.000000</td>\n",
       "      <td>0.712227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.850900</td>\n",
       "      <td>1.129085</td>\n",
       "      <td>1.018592</td>\n",
       "      <td>13401022.000000</td>\n",
       "      <td>0.712414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.875200</td>\n",
       "      <td>1.123271</td>\n",
       "      <td>1.038190</td>\n",
       "      <td>14336381.000000</td>\n",
       "      <td>0.712928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>1.115813</td>\n",
       "      <td>1.039076</td>\n",
       "      <td>15277559.000000</td>\n",
       "      <td>0.714817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.845600</td>\n",
       "      <td>1.109624</td>\n",
       "      <td>1.035711</td>\n",
       "      <td>16239079.000000</td>\n",
       "      <td>0.716518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.855800</td>\n",
       "      <td>1.105260</td>\n",
       "      <td>1.037385</td>\n",
       "      <td>17204163.000000</td>\n",
       "      <td>0.717007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>1.102579</td>\n",
       "      <td>1.021424</td>\n",
       "      <td>18150523.000000</td>\n",
       "      <td>0.717995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>1.096594</td>\n",
       "      <td>1.018537</td>\n",
       "      <td>19071370.000000</td>\n",
       "      <td>0.718882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.772700</td>\n",
       "      <td>1.100511</td>\n",
       "      <td>1.012867</td>\n",
       "      <td>20085247.000000</td>\n",
       "      <td>0.719228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>1.116103</td>\n",
       "      <td>0.976473</td>\n",
       "      <td>21039219.000000</td>\n",
       "      <td>0.718003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.719800</td>\n",
       "      <td>1.116811</td>\n",
       "      <td>0.962961</td>\n",
       "      <td>22017641.000000</td>\n",
       "      <td>0.718618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.738900</td>\n",
       "      <td>1.115453</td>\n",
       "      <td>0.974815</td>\n",
       "      <td>22950583.000000</td>\n",
       "      <td>0.718519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>1.113492</td>\n",
       "      <td>0.974971</td>\n",
       "      <td>23889133.000000</td>\n",
       "      <td>0.718502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>1.113256</td>\n",
       "      <td>0.971307</td>\n",
       "      <td>24831076.000000</td>\n",
       "      <td>0.719875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>1.114408</td>\n",
       "      <td>0.977238</td>\n",
       "      <td>25805262.000000</td>\n",
       "      <td>0.718191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.750900</td>\n",
       "      <td>1.112925</td>\n",
       "      <td>0.976792</td>\n",
       "      <td>26737652.000000</td>\n",
       "      <td>0.718733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.738000</td>\n",
       "      <td>1.111489</td>\n",
       "      <td>0.975877</td>\n",
       "      <td>27689565.000000</td>\n",
       "      <td>0.719091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.753600</td>\n",
       "      <td>1.111309</td>\n",
       "      <td>0.976023</td>\n",
       "      <td>28635073.000000</td>\n",
       "      <td>0.719251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>1.111621</td>\n",
       "      <td>0.977106</td>\n",
       "      <td>29547349.000000</td>\n",
       "      <td>0.718966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "✅ Fine-tuning 완료!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 7. Trainer Configuration 및 학습\n",
    "# ============================================================\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=format_prompt,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Fine-tuning 시작!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✅ Fine-tuning 완료!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6qKC5BAZ84dT",
   "metadata": {
    "id": "6qKC5BAZ84dT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1de44207-0726-4bee-8727-a1712bdd0535"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "✓ Save Model 완료: /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final\n",
      "저장 파일 목록:\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/README.md\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/adapter_config.json\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/adapter_model.safetensors\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/chat_template.jinja\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/special_tokens_map.json\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/tokenizer.json\n",
      "- /content/drive/MyDrive/fine_tuned_models/kanana-legal-finetuned/final/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 8. Save Model\n",
    "# ============================================================\n",
    "\n",
    "output_dir = f\"{OUTPUT_DIR}/final\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"\\n✓ Save Model 완료: {output_dir}\")\n",
    "print(\"저장 파일 목록:\")\n",
    "for saved_path in sorted(Path(output_dir).glob(\"**/*\")):\n",
    "    if saved_path.is_file():\n",
    "        print(f\"- {saved_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "-Hsuy5cN84dT",
   "metadata": {
    "id": "-Hsuy5cN84dT",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fbece97a-1fce-4618-aea7-663be468b524"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "============================================================\n",
      "Model Test\n",
      "============================================================\n",
      "\n",
      "[Test 1]\n",
      "Question: 계약 해제와 계약 해지의 차이를 설명해주세요.\n",
      "\n",
      "Answer:\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "계#############################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 2]\n",
      "Question: 민법상 소멸시효는 무엇인가요?\n",
      "\n",
      "Answer:\n",
      "권#############################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Test 3]\n",
      "Question: 형법상 정당방위의 요건은 무엇인가요?\n",
      "\n",
      "Answer:\n",
      "정#############################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################################\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "🎉 모든 작업 완료!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 9. Test\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Model Test\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    ")\n",
    "\n",
    "test_questions = [\n",
    "    \"계약 해제와 계약 해지의 차이를 설명해주세요.\",\n",
    "    \"민법상 소멸시효는 무엇인가요?\",\n",
    "    \"형법상 정당방위의 요건은 무엇인가요?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    prompt = f\"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "    print(f\"\\n[Test {i}]\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"\\nAnswer:\")\n",
    "\n",
    "    result = pipe(prompt)[0]['generated_text']\n",
    "    answer = result.split(\"### Answer:\")[-1].strip()\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "print(\"\\n🎉 모든 작업 완료!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}