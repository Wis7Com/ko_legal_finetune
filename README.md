# Korean Legal LLM Fine-tuning with QLoRA

Fine-tuning **Kanana Nano 2.1B Instruct** model on Korean legal terminology using **QLoRA** (Quantized Low-Rank Adaptation) for efficient training on consumer GPUs.

## ğŸ¯ Overview

This repository contains a complete fine-tuning pipeline for adapting the Kakao's Kanana Nano 2.1B Instruct model to Korean legal domain using parameter-efficient fine-tuning techniques.

### Key Features

- âœ… **4-bit Quantization**: Efficient memory usage with NF4 quantization
- âœ… **QLoRA**: Low-rank adaptation for parameter-efficient fine-tuning
- âœ… **Optimized for Colab**: Runs on free Google Colab T4 GPU
- âœ… **Production-ready**: Includes evaluation and model saving

## ğŸ“Š Model & Dataset

### Base Model
- **Model**: [kakaocorp/kanana-nano-2.1b-instruct](https://huggingface.co/kakaocorp/kanana-nano-2.1b-instruct)
- **Size**: 2.1B parameters
- **Architecture**: Transformer-based causal language model
- **Language**: Korean

### Dataset
- **Dataset**: [flyingcarycoder/korean-legal-terminology](https://huggingface.co/datasets/flyingcarycoder/korean-legal-terminology)
- **Samples**: 17,484 legal term definitions
- **Format**: Instruction-following (input/output pairs)
- **Domain**: Korean legal terminology and concepts

## ğŸ”§ Fine-tuning Configuration

### QLoRA Settings

```python
# 4-bit Quantization
- Quantization Type: NF4 (Normal Float 4-bit)
- Compute dtype: bfloat16
- Double Quantization: Enabled

# LoRA Configuration
- LoRA Rank (r): 16
- LoRA Alpha: 32
- Target Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- LoRA Dropout: 0.05
- Trainable Parameters: ~23M (1.95% of total)
```

### Training Hyperparameters

```python
# Training Setup
- Epochs: 3
- Batch Size: 4 (per device)
- Gradient Accumulation Steps: 4
- Effective Batch Size: 16

# Optimization
- Optimizer: Paged AdamW 8-bit
- Learning Rate: 2e-4
- LR Scheduler: Cosine
- Warmup Ratio: 0.03
- Weight Decay: 0.01
- Max Gradient Norm: 0.3

# Precision
- Mixed Precision: bfloat16
- Max Sequence Length: 2048
```

## ğŸš€ Quick Start

### 1. Open in Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Wis7com/ko_legal_finetune/blob/main/notebooks/finetune_kanana_legal.ipynb)

### 2. Run the Notebook

The notebook includes:
1. **Environment Setup**: Install dependencies
2. **Data Loading**: Load dataset from Hugging Face
3. **Model Loading**: Load base model with 4-bit quantization
4. **Training**: Fine-tune with QLoRA
5. **Evaluation**: Validate on test set
6. **Saving**: Save to Google Drive

### 3. Local Setup (Optional)

```bash
# Clone repository
git clone https://github.com/Wis7com/ko_legal_finetune.git
cd ko_legal_finetune

# Install dependencies
pip install -r requirements.txt

# Run notebook
jupyter notebook notebooks/finetune_kanana_legal.ipynb
```

## ğŸ’» Hardware Requirements

### Minimum (Google Colab Free)
- **GPU**: T4 (16GB VRAM)
- **RAM**: 12GB
- **Training Time**: ~3-4 hours

### Recommended
- **GPU**: A100 (40GB/80GB VRAM)
- **RAM**: 32GB+
- **Training Time**: ~1-2 hours

### Memory Optimization

The QLoRA approach reduces memory requirements:
- **Full Fine-tuning**: ~40GB VRAM required
- **QLoRA (4-bit)**: ~8-10GB VRAM required
- **Memory Savings**: ~75% reduction

## ğŸ“ˆ Training Results

After fine-tuning, the model achieves:
- Better understanding of Korean legal terminology
- Improved accuracy on legal concept explanations
- Maintained general Korean language capabilities

## ğŸ” Model Architecture

```
Base Model: Kanana Nano 2.1B Instruct
â”œâ”€â”€ Quantization: 4-bit NF4
â”œâ”€â”€ LoRA Adapters (trainable)
â”‚   â”œâ”€â”€ Attention Layers: q_proj, k_proj, v_proj, o_proj
â”‚   â””â”€â”€ MLP Layers: gate_proj, up_proj, down_proj
â””â”€â”€ Base Model (frozen)
```

## ğŸ“ Prompt Format

The model is trained on the following instruction format:

```
### ì§ˆë¬¸:
ë‹¤ìŒ ë²•ë¥  ìš©ì–´(í•œì: å¸æ”¶åˆå€‚)ë¥¼ ì„¤ëª…í•´ì¤˜: í¡ìˆ˜í•©ë³‘

### ë‹µë³€:
ë²•ë¥ ì´ ì •í•˜ëŠ” ì ˆì°¨ì— ì˜í•˜ì—¬ 2 ì´ìƒì˜ ë²•ì¸ ì „ë¶€ ë˜ëŠ” ê·¸ì¤‘ 1ê°œì˜ ë²•ì¸ì´ì™¸ì˜ ë²•ì¸ì´ í•´ì‚°í•˜ì—¬...
```

## ğŸ› ï¸ Technical Details

### Why QLoRA?

1. **Memory Efficient**: 4-bit quantization reduces memory by 75%
2. **Performance**: Minimal accuracy loss compared to full fine-tuning
3. **Accessible**: Enables training on consumer GPUs
4. **Fast**: Reduced computation requirements

### Optimization Techniques

- **Gradient Checkpointing**: Reduces memory during backpropagation
- **Paged Optimizers**: Efficient memory management for optimizer states
- **Mixed Precision**: bfloat16 for faster computation
- **Gradient Accumulation**: Simulates larger batch sizes

## ğŸ“š Citation

If you use this fine-tuning pipeline, please cite:

```bibtex
@misc{ko_legal_finetune_2026,
  title={Korean Legal LLM Fine-tuning with QLoRA},
  author={flyingcarycoder},
  year={2026},
  publisher={GitHub},
  howpublished={\url{https://github.com/Wis7com/ko_legal_finetune}}
}
```

### Dataset Citation

```bibtex
@dataset{korean_legal_terminology_2026,
  title={Korean Legal Terminology Dataset},
  author={flyingcarycoder},
  year={2026},
  publisher={Hugging Face},
  howpublished={\url{https://huggingface.co/datasets/flyingcarycoder/korean-legal-terminology}}
}
```

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“§ Contact

For questions or issues, please open an issue on GitHub.

## ğŸ™ Acknowledgments

- **Base Model**: [Kakao Corp](https://huggingface.co/kakaocorp) for Kanana Nano 2.1B Instruct
- **Dataset**: Korean Legal Terminology dataset contributors
- **QLoRA**: [Tim Dettmers et al.](https://arxiv.org/abs/2305.14314) for the QLoRA method
- **Libraries**: HuggingFace Transformers, PEFT, TRL, bitsandbytes
